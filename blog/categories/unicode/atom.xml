<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Unicode | In Pursuit of Laziness]]></title>
  <link href="http://manishearth.github.io/blog/categories/unicode/atom.xml" rel="self"/>
  <link href="http://manishearth.github.io/"/>
  <updated>2017-05-16T00:06:27+00:00</updated>
  <id>http://manishearth.github.io/</id>
  <author>
    <name><![CDATA[Manish Goregaokar]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Breaking Our Latin-1 Assumptions]]></title>
    <link href="http://manishearth.github.io/blog/2017/01/15/breaking-our-latin-1-assumptions/"/>
    <updated>2017-01-15T00:00:00+00:00</updated>
    <id>http://manishearth.github.io/blog/2017/01/15/breaking-our-latin-1-assumptions</id>
    <content type="html"><![CDATA[<p>So in my <a href="http://manishearth.github.io/blog/2017/01/14/stop-ascribing-meaning-to-unicode-code-points">previous post</a> I explored a specific (wrong) assumption that programmers
tend to make about the nature of code points and text.</p>

<p>I was asked multiple times about other assumptions we tend to make. There are a lot. Most
Latin-based scripts are simple, but most programmers spend their time dealing with Latin
text so these complexities never come up.</p>

<p>I thought it would be useful to share my personal list of
<a href="https://twitter.com/ManishEarth/status/810582690906931200">scripts that break our Latin-1 assumptions</a>. This is a list I mentally check against
whenever I am attempting to reason about text. I check if I&rsquo;m making any assumptions that
break in these scripts. <em>Most</em> of these concepts are independent of Unicode; so any program
would have to deal with this regardless of encoding.</p>

<p>I again recommend going through <a href="https://eev.ee/blog/2015/09/12/dark-corners-of-unicode/">eevee&rsquo;s post</a>, since it covers many related issues.
<a href="https://github.com/jagracey/Awesome-Unicode">Awesome-Unicode</a> also has a lot of random tidbits about Unicode.</p>

<p>Anyway, here&rsquo;s the list. Note that a lot of the concepts here exist in scripts other than the
ones listed, these are just the scripts <em>I</em> use for comparing.</p>

<h2>Arabic / Hebrew</h2>

<p>Both Arabic and Hebrew are RTL scripts; they read right-to-left. This may even affect how
a page is laid out, see the <a href="https://he.wikipedia.org/wiki/%D7%A2%D7%9E%D7%95%D7%93_%D7%A8%D7%90%D7%A9%D7%99">Hebrew Wikipedia</a>.</p>

<p>They both have a concept of letters changing how they look depending on where they are in the word.
Hebrew has the &ldquo;sofit&rdquo; letters, which use separate code points. For example, Kaf (◊õ) should be typed
as ◊ö at the end of a word. Greek has something similar with the sigma.</p>

<p>In Arabic, the letters can have up to four different forms, depending on whether they start a word,
end a word, are inside a word, or are used by themselves. These forms can look very different. They
don&rsquo;t use separate code points for this; however. You can see a list of these forms <a href="https://en.wikipedia.org/wiki/Arabic_alphabet#Table_of_basic_letters">here</a></p>

<p>As I mentioned in the last post, U+FDFD (Ô∑Ω), a ligature representing the Basamala,
is also a character that breaks a lot of assumptions.</p>

<h2>Indic scripts</h2>

<p>Indic scripts are <em>abugidas</em>, where you have consonants with vowel modifiers. For example, ‡§ï is
&ldquo;k…ô&rdquo;, where the upside down &ldquo;e&rdquo; is a schwa, something like an &ldquo;uh&rdquo; vowel sound. You can change the
vowel by adding a diacritic (e.g <code>‡§æ</code>); getting things like ‡§ï‡§æ (&ldquo;kaa&rdquo;) ‡§ï‡•ã (&ldquo;koh&rdquo;) ‡§ï‡•Ç (&ldquo;koo&rdquo;).</p>

<p>You can also mash together consonants to create consonant clusters. The &ldquo;virama&rdquo; is a vowel-killer
symbol that removes the inherent schwa vowel. So, <code>‡§ï</code> + <code>‡•ç</code> becomes <code>‡§ï‡•ç</code>. This sound itself is
unpronounceable since ‡§ï is a stop consonant (vowel-killed consonants can be pronounced for nasal and some other
consonants though), but you can combine it with another consonant, as <code>‡§ï‡•ç</code> + <code>‡§∞</code> (&ldquo;r…ô&rdquo;), to get <code>‡§ï‡•ç‡§∞</code>
(&ldquo;kr…ô&rdquo;). Consonants can be strung up infinitely, and you can stick one or more vowel diacritics
after that. Usually, you won&rsquo;t see more than two consonants in a cluster, but larger ones are not
uncommon in Sanskrit (or when writing down some onomatopoeia). They may not get rendered as single
glyphs, depending on the font.</p>

<p>One thing that crops up is that there&rsquo;s no unambiguous concept of a letter here. There
is a concept of an &ldquo;akshara&rdquo;, which basically includes the vowel diacritics, and
depending on who you talk to may also include consonant clusters. Often things are
clusters an akshara depending on whether they&rsquo;re drawn with an explicit virama
or form a single glyph.</p>

<p>In general the nature of the virama as a two-way combining character in Unicode is pretty new.</p>

<h2>Hangul</h2>

<p>Korean does its own fun thing when it comes to conjoining characters. Hangul has a concept
of a &ldquo;syllable block&rdquo;, which is basically a letter. It&rsquo;s made up of a leading consonant,
medial vowel, and an optional tail consonant. &#x1100;&#x1161;&#x11A8; is an example of
such a syllable block, and it can be typed as &#x1100; + &#x1161; + &#x11A8;. It can
also be typed as &#xAC01;, which is a &ldquo;precomposed form&rdquo; (and a single code point).</p>

<p>These characters are examples of combining characters with very specific combining rules. Unlike
accents or other diacritics, these combining characters will combine with the surrounding characters
only when the surrounding characters form an L-V-T or L-V syllable block.</p>

<p>As I mentioned in my previous post, apparently syllable blocks with more (adjacent) Ls, Vs, and Ts are
also valid and used in Old Korean, so the grapheme segmentation algorithm in Unicode considers
&ldquo;·ÑÄ·ÑÄ·ÑÄ&#x1100;&#x1161;&#x11A8;·Ü®·Ü®&rdquo; to be a single grapheme (<a href="http://www.unicode.org/reports/tr29/#Hangul_Syllable_Boundary_Determination">it explicitly mentions this</a>).
I&rsquo;m not aware of any fonts which render these as a single syllable block, or if that&rsquo;s even
a valid thing to do.</p>

<h2>Han scripts</h2>

<p>So Chinese (Hanzi), Japanese (Kanji<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>), Korean (Hanja<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>), and Vietnamese (H√°n t·ª±, along with Ch·ªØ
N√¥m <sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>) all share glyphs, collectively called &ldquo;Han characters&rdquo; (or CJK characters<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>). These
languages at some point in their history borrowed the Chinese writing system, and made their own
changes to it to tailor to their needs.</p>

<p>Now, the Han characters are ideographs. This is not a phonetic script; individual characters
represent words. The word/idea they represent is not always consistent across languages. The
pronounciation is usually different too. Sometimes, the glyph is drawn slightly differently based on
the language used. There are around 80,000 Han ideographs in Unicode right now.</p>

<p>The concept of ideographs itself breaks some of our Latin-1 assumptions. For example, how
do you define Levenshtein edit distance for text using Han ideographs? The straight answer is that
you can&rsquo;t, though if you step back and decide <em>why</em> you need edit distance you might be able
to find a workaround. For example, if you need it to detect typos, the user&rsquo;s input method
may help. If it&rsquo;s based on pinyin or bopomofo, you might be able to reverse-convert to the
phonetic script, apply edit distance in that space, and convert back. Or not. I only maintain
an idle curiosity in these scripts and don&rsquo;t actually use them, so I&rsquo;m not sure how well this would
work.</p>

<p>The concept of halfwidth character is a quirk that breaks some assumptions.</p>

<p>In the space of Unicode in particular, all of these scripts are represented by a single set of
ideographs. This is known as &ldquo;Han unification&rdquo;. This is a pretty controversial issue, but the
end result is that rendering may sometimes be dependent on the language of the text, which
e.g. in HTML you set with a <code>&lt;span lang=whatever&gt;</code>. <a href="https://en.wikipedia.org/wiki/Han_unification#Examples_of_language-dependent_glyphs">The wiki page</a> has some examples of
encoding-dependent characters.</p>

<p>Unicode also has a concept of variation selector, which is a code point that can be used to
select between variations for a code point that has multiple ways of being drawn. These
do get used in Han scripts.</p>

<p>While this doesn&rsquo;t affect rendering, Unicode, as a system for <em>describing</em> text,
also has a concept of interlinear annotation characters. These are used to represent
<a href="https://en.wikipedia.org/wiki/Ruby_character">furigana / ruby</a>. Fonts don&rsquo;t render this, but it&rsquo;s useful if you want to represent
text that uses ruby. Similarly, there are <a href="https://en.wikipedia.org/wiki/Chinese_character_description_languages#Ideographic_Description_Sequences">ideographic description sequences</a> which
can be used to &ldquo;build up&rdquo; glyphs from smaller ones when the glyph can&rsquo;t be encoded in
Unicode. These, too, are not to be rendered, but can be used when you want to describe
the existence of a character like <a href="https://en.wikipedia.org/wiki/Biangbiang_noodles#Chinese_character_for_bi.C3.A1ng">bi√°ng</a>. These are not things a programmer
needs to worry about; I just find them interesting and couldn&rsquo;t resist mentioning them :)</p>

<p>Japanese speakers haven&rsquo;t completely moved to Unicode; there are a lot of things out there
using Shift-JIS, and IIRC there are valid reasons for that (perhaps Han unification?). This
is another thing you may have to consider.</p>

<p>Finally, these scripts are often written <em>vertically</em>, top-down. <a href="https://en.wikipedia.org/wiki/Mongolian_script">Mongolian</a>, while
not being a Han script, is written vertically sideways, which is pretty unique. The
CSS <a href="https://drafts.csswg.org/css-writing-modes/">writing modes</a> spec introduces various concepts related to this, though that&rsquo;s mostly in the
context of the Web.</p>

<h2>Thai / Khmer / Burmese / Lao</h2>

<p>These scripts don&rsquo;t use spaces to split words. Instead, they have rules for what kinds of sequences
of characters start and end a word. This can be determined programmatically, however IIRC the
Unicode spec does not attempt to deal with this. There are libraries you can use here instead.</p>

<h2>Latin scripts themselves!</h2>

<p>Turkish is a latin-based script. But it has a quirk: The uppercase of &ldquo;i&rdquo; is
a dotted &ldquo;ƒ∞&rdquo;, and the lowercase of &ldquo;I&rdquo; is &ldquo;ƒ±&rdquo;. If doing case-based operations, try to use
a Unicode-aware library, and try to provide the locale if possible.</p>

<p>Also, not all code points have a single-codepoint uppercase version. The eszett (√ü) capitalizes
to &ldquo;SS&rdquo;. There&rsquo;s also the &ldquo;capital&rdquo; eszett ·∫û, but its usage seems to vary and I&rsquo;m not exactly
sure how it interacts here.</p>

<p>While Latin-1 uses precomposed characters, Unicode also introduces ways to specify the same
characters via combining diacritics. Treating these the same involves using the normalization
algorithms (NFC/NFD).</p>

<h2>Emoji</h2>

<p>Well, not a script<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup>. But emoji is weird enough that it breaks many of our assumptions. The
scripts above cover most of these, but it&rsquo;s sometimes easier to think of them
in the context of emoji.</p>

<p>The main thing with emoji is that you can use a zero-width-joiner character to glue emoji together.</p>

<p>For example, the family emoji üë©‚Äçüë©‚Äçüëß‚Äçüë¶ (may not render for you) is made by using the woman/man/girl/boy
emoji and gluing them together with ZWJs. You can see its decomposition in <a href="https://r12a.github.io/uniview/?charlist=%F0%9F%91%A9%E2%80%8D%F0%9F%91%A9%E2%80%8D%F0%9F%91%A7%E2%80%8D%F0%9F%91%A6">uniview</a>.</p>

<p>There are more sequences like this, which you can see in the <a href="http://unicode.org/Public/emoji/4.0/emoji-zwj-sequences.txt">emoji-zwj-sequences</a> file. For
example, MAN + ZWJ + COOK will give a male cook emoji (font support is sketchy).
Similarly, SWIMMER + ZWJ + FEMALE SIGN is a female swimmer. You have both sequences of
the form &ldquo;gendered person + zwj + thing&rdquo;, and &ldquo;emoji containing human + zwj + gender&rdquo;,
IIRC due to legacy issues<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup></p>

<p>There are also <a href="http://www.unicode.org/reports/tr51/#Diversity">modifier characters</a> that let you change the skin tone of an emoji that
contains a human (or human body part, like the hand-gesture emojis) in it.</p>

<p>Finally, the flag emoji are pretty special snowflakes. For example, üá™üá∏ is the Spanish
flag. It&rsquo;s made up of <a href="https://r12a.github.io/uniview/?charlist=%F0%9F%87%AA%F0%9F%87%B8">two regional indicator characters for &ldquo;E&rdquo; and &ldquo;S&rdquo;</a>.</p>

<p>Unicode didn&rsquo;t want to deal with adding new flags each time a new country or territory pops up. Nor
did they want to get into the tricky business of determining what a country <em>is</em>, for example
when dealing with disputed territories. So instead, they just defined these regional indicator
symbols. Fonts are supposed to take pairs of RI symbols<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup> and map the country code to a flag.
This mapping is up to them, so it&rsquo;s totally valid for a font to render a regional indicator
pair &ldquo;E&rdquo; + &ldquo;S&rdquo; as something other than the flag of Spain. On some Chinese systems, for example,
the flag for Taiwan (üáπüáº) may not render.</p>

<hr />

<p>I hightly recommend comparing against this relatively small list of scripts the next time you
are writing code that does heavy manipulation of user-provided strings.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>Supplemented (but not replaced) by the Hiragana and Katakana phonetic scripts. In widespread use.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>Replaced by Hangul in modern usage<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>Replaced by ch·ªØ qu·ªëc ng·ªØ in modern usage, which is based on the Latin alphabet<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>&ldquo;CJK&rdquo; (Chinese-Japanese-Korean) is probably more accurate here, though it probably should include &ldquo;V&rdquo; for Vietnamese too. Not all of these ideographs come from Han; the other scripts invented some of their own. See: Kokuji, Gukja, Ch·ªØ N√¥m.<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
<li id="fn:5">
<p>Back in <em>my</em> day we painstakingly typed actual real words on numeric phone keypads, while trudging to üè´ in three feet of ‚ùÑÔ∏èÔ∏è, and it was uphill both ways, and we weren&rsquo;t even <em>allowed</em> üì±s in üè´. Get off my lawn!<a href="#fnref:5" rev="footnote">&#8617;</a></p></li>
<li id="fn:6">
<p>We previously had individual code points for professions and stuff and they decided to switch over to using existing object emoji with combiners instead of inventing new profession emoji all the time<a href="#fnref:6" rev="footnote">&#8617;</a></p></li>
<li id="fn:7">
<p>676 countries should be enough for anybody<a href="#fnref:7" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Let's Stop Ascribing Meaning to Code Points]]></title>
    <link href="http://manishearth.github.io/blog/2017/01/14/stop-ascribing-meaning-to-unicode-code-points/"/>
    <updated>2017-01-14T00:00:00+00:00</updated>
    <id>http://manishearth.github.io/blog/2017/01/14/stop-ascribing-meaning-to-unicode-code-points</id>
    <content type="html"><![CDATA[<p><em>Update: This post got a sequel, <a href="http://manishearth.github.io/blog/2017/01/15/breaking-our-latin-1-assumptions/">Breaking our latin-1 assumptions</a>.</em></p>

<p>I&rsquo;ve seen misconceptions about Unicode crop up regularly in posts discussing it. One very common
misconception I&rsquo;ve seen is that <em>code points have cross-language intrinsic meaning</em>.</p>

<p>It usually comes up when people are comparing UTF8 and UTF32. Folks start implying that code points
<em>mean</em> something, and that O(1) indexing or slicing at code point boundaries is a useful operation.
I&rsquo;ve also seen this assumption manifest itself in actual programs which make incorrect assumptions
about the nature of code points and mess things up when fed non-Latin text.</p>

<p>If you like reading about unicode, you might also want to go through <a href="https://eev.ee/blog/2015/09/12/dark-corners-of-unicode/">Eevee&rsquo;s article</a>
on the dark corners of unicode. Great read!</p>

<h2>Encodings</h2>

<p>So, anyway, we have some popular encodings for Unicode. UTF8 encodes 7-bit code points as a single
byte, 11-bit code points as two bytes, 16-bit code points as 3 bytes, and 21-bit code points as four
bytes. UTF-16 encodes the first three in two bytes, and the last one as four bytes (logically, a
pair of two-byte code units). UTF-32 encodes all code points as 4-byte code units. UTF-16 is mostly
a &ldquo;worst of both worlds&rdquo; compromise at this point, and the main programming language I can think of
that uses it (and exposes it in this form) is Javascript, and that too in a broken way.</p>

<p>The nice thing about UTF8 is that it saves space. Of course, that is subjective and dependent on
the script you use most commonly, for example my first name is 12 bytes in UTF-8 but only 4
in ISCII (or a hypothetical unicode-based encoding that swapped the Devanagri Unicode block with
the ASCII block). It also uses more space over the very non-hypothetical UTF-16 encoding if you
tend to use code points in the U+0800 - U+FFFF range. It always uses less space than UTF-32 however.</p>

<p>A commonly touted disadvantage of UTF-8 is that string indexing is <code>O(n)</code>. Because code points take
up a variable number of bytes, you won&rsquo;t know where the 5th codepoint is until you scan the string
and look for it. UTF-32 doesn&rsquo;t have this problem; it&rsquo;s always <code>4 * index</code> bytes away.</p>

<p>The problem here is that indexing by code point shouldn&rsquo;t be an operation you ever need!</p>

<h2>Indexing by code point</h2>

<p>The main time you want to be able to index by code point is if you&rsquo;re implementing algorithms
defined in the unicode spec that operate on unicode strings (casefolding, segmentation, NFD/NFC).
Most if not all of these algorithms operate on whole strings, so implementing them
as an iteration pass is usually necessary anyway, so you don&rsquo;t lose anything if you can&rsquo;t
do arbitrary code point indexing.</p>

<p>But for application logic, dealing with code points doesn&rsquo;t really make sense. This is because
code points have no intrinsic meaning. They are not &ldquo;characters&rdquo;. I&rsquo;m using scare quotes here
because a &ldquo;character&rdquo; isn&rsquo;t a well-defined concept either, but we&rsquo;ll get to that later.</p>

<p>For example, &ldquo;e&#x0301;&rdquo; is two code points (<code>e</code> +<code>ÃÅ</code>), where one of them is a combining accent. My name,
&ldquo;‡§Æ‡§®‡•Ä‡§∑&rdquo;, visually looks like three &ldquo;characters&rdquo;, but is four code points. The &ldquo;‡§®‡•Ä&rdquo; is made up of <code>‡§®</code>
+ <code>‡•Ä</code>. My last name contains a &ldquo;character&rdquo; made up of three code points (and multiple two-code-point
&ldquo;characters&rdquo;). The flag emoji &ldquo;üá∫üá∏&rdquo; is also made of two code points, <code>üá∫</code> + <code>üá∏</code>.</p>

<p>One false assumption that&rsquo;s often made is that code points are a single column wide. They&rsquo;re not.
They sometimes bunch up to form characters that fit in single &ldquo;columns&rdquo;. This is often dependent on
the font, and if your application relies on this, you should be querying the font. There are even
code points like U+FDFD (Ô∑Ω) which are often rendered multiple columns wide. In fact, in my
<em>monospace</em> font in my text editor, that character is rendered <em>almost</em> 12 columns wide. Yes,
&ldquo;almost&rdquo;, subsequent characters get offset a tiny bit. I don&rsquo;t know why.</p>

<p>Another false assumption is that editing actions (selection, backspace, cut, paste) operate on code
points. In both Chrome and Firefox, selection will often include multiple code points. All the
multi-code-point examples I gave above fall into this category. An interesting testcase for this is
the string &ldquo;·ÑÄ·ÑÄ·ÑÄÍ∞Å·Ü®·Ü®&rdquo;, which will rarely if ever render as a single &ldquo;character&rdquo; but will be considered
as one for the purposes of selection, pretty much universally. I&rsquo;ll get to why this is later.</p>

<p>Backspace can gobble multiple code points at once too, but the heuristics are different. The reason
behind this is that backspace needs to mirror the act of typing, and while typing sometimes
constructs multi-codepoint characters, backspace decomposes it piece by piece. In cases where a
multi-codepoint &ldquo;character&rdquo; <em>can</em> be logically decomposed (e.g. &ldquo;letter + accent&rdquo;), backspace will
decompose it, by removing the accent or whatever. But some multi-codepoint characters are not
&ldquo;constructions&rdquo; of general concepts that should be exposed to the user. For example, a user should
never need to know that the &ldquo;üá∫üá∏&rdquo; flag emoji is made of <code>üá∫</code> + <code>üá∏</code>, and hitting backspace on it should
delete both codepoints. Similarly, variation selectors and other such code points shouldn&rsquo;t
be treated as their own unit when backspacing.</p>

<p>On my Mac most builtin apps (which I presume use the OSX UI toolkits) seem to use the same
heuristics that Firefox/Chrome use for selection for both selection and backspace. While the
treatment of code points in editing contexts is not consistent, it seems like applications
consistently do not consider code points as &ldquo;editing units&rdquo;.</p>

<p>Now, it is true that you often need <em>some</em> way to index a string. For example, if you have a large
document and need to represent a slice of it. This could be a user-selection, or something delimeted
by markup. Basically, you&rsquo;ve already gone through the document and have a section you want to be
able to refer to later without copying it out.</p>

<p>However, you don&rsquo;t need code point indexing here, byte
indexing works fine! UTF8 is designed so that you can check if you&rsquo;re on a code point boundary even
if you just byte-index directly. It does this by restricting the kinds of bytes allowed. One-byte
code points never have the high bit set (ASCII). All other code points have the high bit set in each
byte. The first byte of multibyte codepoints always starts with a sequence that specifies the number
of bytes in the codepoint, and such sequences can&rsquo;t be found in the lower-order bytes of any
multibyte codepoint. You can see this visually in the table <a href="https://en.wikipedia.org/wiki/UTF-8#Description">here</a>. The upshot of all this
is that you just need to check the current byte if you want to be sure you&rsquo;re on a codepoint
boundary, and if you receive an arbitrarily byte-sliced string, you will not mistake it for
something else. It&rsquo;s not possible to have a valid code point be a subslice of another, or form a
valid code point by subslicing a sequence of two different ones by cutting each in half.</p>

<p>So all you need to do is keep track of the byte indices, and use them for slicing it later.</p>

<p>All in all, it&rsquo;s important to always remember that &ldquo;code point&rdquo; doesn&rsquo;t have intrinsic meaning. If
you need to do a segmentation operation on a string, find out what <em>exactly</em> you&rsquo;re looking for, and
what concept maps closest to that. It&rsquo;s rare that &ldquo;code point&rdquo; is the concept you&rsquo;re looking for.
In <em>most</em> cases, what you&rsquo;re looking for instead is &ldquo;grapheme cluster&rdquo;.</p>

<h2>Grapheme clusters</h2>

<p>The concept of a &ldquo;character&rdquo; is a nebulous one. Is &ldquo;&#x1100;&#x1161;&#x11A8;&rdquo; a single character, or
three? How about &ldquo;‡§®‡•Ä&rdquo;? Or &ldquo;‡Æ®‡Æø&rdquo;? Or the &ldquo;üë®‚Äç‚ù§Ô∏è‚Äçüë®&rdquo; emoji<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>? Or the &ldquo;üë®‚Äçüë®‚Äçüëß‚Äçüëß&rdquo; family emoji<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup>?
Different scripts have different concepts which may not clearly map to the Latin notion of &ldquo;letter&rdquo;
or our programmery notion of &ldquo;character&rdquo;.</p>

<p>Unicode itself gives the term <a href="http://unicode.org/glossary/#character">&ldquo;character&rdquo;</a> multiple incompatible meanings, and as
far as I know doesn&rsquo;t use the term in any normative text.</p>

<p>Often, you need to deal with what is actually displayed to the user. A lot of terminal emulators do
this wrong, and end up messing up cursor placement. I used to use irssi-xmpp to keep my Facebook and
Gchat conversations in my IRC client, but I eventually stopped as I was increasingly chatting in
Marathi or Hindi and I prefer using the actual script over romanizing<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>, and it would just break
my terminal<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>. Also, they got rid of the XMPP bridge but I&rsquo;d already cut down on it by then.</p>

<p>So sometimes, you need an API querying what the font is doing. Generally, when talking about the
actual rendered image, the term &ldquo;glyph&rdquo; or &ldquo;glyph image&rdquo; is used.</p>

<p>However, you can&rsquo;t always query the font. Text itself exists independent of rendering, and sometimes
you need a rendering-agnostic way of segmenting it into &ldquo;characters&rdquo;.</p>

<p>For this, Unicode has a concept of <a href="http://unicode.org/glossary/#grapheme_cluster">&ldquo;grapheme cluster&rdquo;</a>. There&rsquo;s also &ldquo;extended grapheme
cluster&rdquo; (EGC), which is basically an updated version of the concept. In this post, whenever
I use the term &ldquo;grapheme cluster&rdquo;, I am talking about EGCs.</p>

<p>The term is defined and explored in <a href="http://www.unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries">UAX #29</a>. It starts by pinning down the still-nebulous
concept of &ldquo;user-perceived character&rdquo; (&ldquo;a basic unit of a writing system for a language&rdquo;),
and then declares the concept of a &ldquo;grapheme cluster&rdquo; to be an approximation to this notion
that we can determine programmatically.</p>

<p>A rough definition of grapheme cluster is a &ldquo;horizontally segmentable unit of text&rdquo;.</p>

<p>The spec goes into detail as to the exact algorithm that segments text at grapheme cluster
boundaries. All of the examples I gave in the first paragraph of this section are single grapheme
clusters. So is &ldquo;·ÑÄ·ÑÄ·ÑÄÍ∞Å·Ü®·Ü®&rdquo; (or &ldquo;·ÑÄ·ÑÄ·ÑÄ&#x1100;&#x1161;&#x11A8;·Ü®·Ü®&rdquo;), which apparently is considered a
single syllable block in Hangul even though it is not of the typical form of leading consonant +
vowel + optional tail consonant, but is not something you&rsquo;d see in modern Korean. The spec
explicitly talks of this case so it seems to be on purpose. I like this string because nothing I
know of renders it as a single glyph; so you can easily use it to tell if a particular segmentation-
aware operation uses grapheme clusters as segmentation. If you try and select it, in most browsers
you will be forced to select the whole thing, but backspace will delete the jamos one by one. For
the second string, backspace will decompose the core syllable block too (in the first string the
syllable block &#x1100;&#x1161;&#x11A8; is &ldquo;precomposed&rdquo; as a single code point, in the second one I
built it using combining jamos).</p>

<p>Basically, unless you have very specific requirements or are able to query the font, use an API that
segments strings into grapheme clusters wherever you need to deal with the notion of &ldquo;character&rdquo;.</p>

<h2>Language defaults</h2>

<p>Now, a lot of languages by default are now using Unicode-aware encodings. This is great. It gets rid
of the misconception that characters are one byte long.</p>

<p>But it doesn&rsquo;t get rid of the misconception that user-perceived characters are one code point long.</p>

<p>There are only two languages I know of which handle this well: Swift and Perl 6. I don&rsquo;t know much
about Perl 6&rsquo;s thing so I can&rsquo;t really comment on it, but I am really happy with what Swift does:</p>

<p>In Swift, the <code>Character</code> type is an extended grapheme cluster. This does mean that a
character itself is basically a string, since EGCs can be arbitrarily many code points long.</p>

<p>All the APIs by default deal with EGCs. The length of a string is the number of EGCs in it. They
are indexed by EGC. Iteration yields EGCs. The default comparison algorithm uses unicode
canonical equivalence, which I think is kind of neat. Of course, APIs that work with code
points are exposed too, you can iterate over the code points using <code>.unicodeScalars</code>.</p>

<p>The internal encoding itself is &hellip; weird (and as far as I can tell not publicly exposed), but as a
higher level language I think it&rsquo;s fine to do things like that.</p>

<p>I strongly feel that languages should be moving in this direction, having defaults involving
grapheme clusters.</p>

<p>Rust, for example, gets a lot of things right &ndash; it has UTF-8 strings. It internally uses byte
indices in slices. Explicit slicing usually uses byte indices too, and will panic if out of bounds.
The non-O(1) methods are all explicit, since you will use an iterator to perform the operation (E.g.
<code>.chars().nth(5)</code>). This encourages people to <em>think</em> about the cost, and it also  encourages people
to coalesce the cost with nearby iterations &ndash; if you are going to do multiple <code>O(n)</code> things, do
them in a single iteration! Rust <code>char</code>s represent code points. <code>.char_indices()</code> is
a useful string iteration method that bridges the gap between byte indexing and code points.</p>

<p>However, while the documentation does mention grapheme clusters, the stdlib is not aware of the
concept of grapheme clusters at all. The default &ldquo;fundamental&rdquo; unit of the string in Rust is
a code point, and the operations revolve around that. If you want grapheme clusters, you
may use <a href="https://unicode-rs.github.io/unicode-segmentation/unicode_segmentation/trait.UnicodeSegmentation.html#tymethod.graphemes"><code>unicode-segmentation</code></a></p>

<p>Now, Rust is a systems programming language and it just wouldn&rsquo;t do to have expensive grapheme
segmentation operations all over your string defaults. I&rsquo;m very happy that the expensive <code>O(n)</code>
operations are all only possible with explicit acknowledgement of the cost. So I do think that going
the Swift route would be counterproductive for Rust. Not that it <em>can</em> anyway, due to backwards
compatibility :)</p>

<p>But I would prefer if the grapheme segmentation methods were in the stdlib (they used to be).
This is probably not something that will happen, though I should probably push for the unicode
crates being move into the nursery at least.</p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>Emoji may not render as a single glyph depending on the font.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
<li id="fn:2">
<p>While writing this paragraph I discovered that wrapping text that contains lots of family emoji hangs Sublime. Neat.<a href="#fnref:2" rev="footnote">&#8617;</a></p></li>
<li id="fn:3">
<p>Part of the reason here is that I just find romanization confusing. There are some standardized ways to romanize which don&rsquo;t get used much. My friends and I romanize one way, different from the standardizations. My family members romanize things a completely different way and it&rsquo;s a bit hard to read. Then again, romanization <em>does</em> hide the fact that my spelling in Hindi is atrocious :)<a href="#fnref:3" rev="footnote">&#8617;</a></p></li>
<li id="fn:4">
<p>It&rsquo;s possible to make work. You need a good terminal emulator, with the right settings, the right settings in your env vars, the right settings in irssi, and the right settings in screen. I think my current setup works well with non-ascii text but I&rsquo;m not sure what I did to make it happen.<a href="#fnref:4" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
</feed>
